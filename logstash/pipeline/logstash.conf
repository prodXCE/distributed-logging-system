# logstash/pipeline/logstash.conf

# ================= INPUT =================
input {
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["logs-topic"]
    group_id => "logstash-consumer-group"

    codec => "json"
  }
}

# ================= FILTER =================
filter {

  if "transactionId=" in [rawMessage] {
    kv {
      source => "rawMessage"
      field_split => " "
      value_split => "="
    }
  }

  else if [rawMessage] =~ "(?m)^(INFO|WARN|ERROR|DEBUG)" {
    grok {
      match => { "rawMessage" => "(?m)^%{LOGLEVEL:level_parsed} \[%{DATA:thread_name}\] %{JAVACLASS:class_name} - %{GREEDYDATA:message_parsed}" }

      overwrite => ["level", "message"]
    }
  }

  # --- TIMESTAMP CORRECTION ---
  date {
    match => [ "timestamp", "ISO8601" ]
    remove_field => ["timestamp"]
  }

  # --- CLEANUP ---
  # The mutate filter can add, remove, or change fields.
  mutate {
  }
}

# ================= OUTPUT =================
output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }

  stdout {
    codec => rubydebug
  }
}