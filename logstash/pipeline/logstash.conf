# logstash/pipeline/logstash.conf

# ========== INPUT ===========
input {
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["logs-topic"]
    group_id => "logstash-consumer-group"
    codec => "json"
    codec => multiline {
      pattern => "^(INFO|WARN|ERROR|DEBUG)"
      negate => true
      what => "previous"
      auto_flush_interval => 2
    }
  }
}

# ========== FILTER ===========
filter {
  if "transactionId=" in [rawMessage] {
     kv {
        source => "rawMessage"
        field_split => " "
        volue_split => "="
        remove_field => ["rawMessage"]
     }
  }

  else if [rawMessage] =~ "^(INFO|WARN|ERROR|DEBUG)" {
     grok {
      match => { "rawMessage" => "%{LOGLEVEL:level_parsed} \[%{DATA:thread_name}\] %{JAVACLASS:class_name} - %{GREEDYDATA:message_parsed}" }
      overwrite => ["level", "message"]
     }
  }

  date {
    match => [ "timestamp", "ISO8601" ]
    remove_field => ["timestamp"]
  }

  mutate {
    remove_field => ["rawMessage"]
  }
}

output {
  elasticsearch {
     hosts => ["https://elasticsearch:9200"]
     index => "logs-%{+YYYY.MM.dd}"
  }

  stdout {
    codec => rubydebug
  }
}